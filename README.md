# Sa√©503 - Orchestrer la conteneurisation d‚Äôune application 
Elabor√© par **Loris Egault** & **Simon Rageul** ***(Bin√¥me 5)***
##### üìÖ 23 janvier 2025
#### Promotion BUT3 FA *2024-2025*
---
## Lot 1 : D√©finition de l'architecture
![Diagramme de l'architecture technique](Sch√©maSAE503.svg)
## Lot 2 : Mise en place et configuration d'un orchestrateur de conteneurs

### Liens utilis√©s

##### Installation de docker :
<a href="https://docs.docker.com/engine/install/debian/">
  <img src="imgs/docker.webp" width="50">
</a>
https://docs.docker.com/engine/install/debian/

##### Installation de minikube :
<a href="https://kubernetes.io/fr/docs/tasks/tools/install-minikube/">
  <img src="imgs/minikube.png" width="50">
</a>
https://kubernetes.io/fr/docs/tasks/tools/install-minikube/

##### Installation de kubectl :
<a href="https://kubernetes.io/fr/docs/tasks/tools/install-minikube/">
  <img src="imgs/kubectl.svg" width="50">
</a>
https://kubernetes.io/fr/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux

### Commandes bash
Ajoute l'utilisateur actuel au groupe Docker et applique les changements imm√©diatement :
```bash
sudo usermod -aG docker $USER && newgrp docker
```
Commande pour se mettre en **root (avec minikube)**
```bash
root@Minikube:/home/user# apt install sudo
```
D√©marrer minikube
```bash
@Minikube:~$ minikube start
```
##### Cr√©ation de 2 namespaces correspondant aux 2 plateformes qualification et production : 
```bash
user@Minikube:~$ kubectl create namespace qualification
namespace/qualification created

user@Minikube:~$ kubectl create namespace production
namespace/production created
```
## Lot 3 : Refactorisation de l‚Äôapplication
#### Installation de python sur la VM pour tester ind√©pendamment nos api (et installer les modules : flask, redis, swagger *et* wraps)
## *Objectifs*
- D√©couper l'application monolithique en trois microservices distincts pour chaque fonctionnalit√© principale : /users, /quotes, et /search.
- Respecter les bonnes pratiques en mati√®re de conteneurisation, telles que l'utilisation d'images Docker minimales et la variabilisation des param√®tres.
- Assurer la compatibilit√© avec l'orchestrateur pour un d√©ploiement simplifi√© et performant.
### √âtapes de refactorisation
#### D√©coupage en trois microservices

Le code Python fourni a √©t√© adapt√© pour √™tre divis√© en trois fichiers distincts correspondant aux points d'acc√®s :

- /users : gestion des utilisateurs (ajout, r√©cup√©ration).
- /quotes : gestion des citations (ajout, suppression, r√©cup√©ration).
- /search : recherche de citations par mot-cl√©.

Chaque microservice a son propre fichier Flask et fonctionne ind√©pendamment.
### Cr√©ation des Dockerfiles

Chaque microservice dispose d'un fichier Dockerfile optimis√© :

#### Exemple de Dockerfile pour /users :

##### Utiliser une image Python minimale
```python
FROM python:3.10-slim
```
##### D√©finir le r√©pertoire de travail
```bash
WORKDIR /app
```
##### Copier les fichiers n√©cessaires
```bash
COPY requirements.txt .
COPY users_service.py .
```
##### Installer les d√©pendances
```python
RUN pip install --no-cache-dir -r requirements.txt
```
##### Exposer le port
```bash   
EXPOSE 5000
```
##### Lancer l'application (dockerfile)
```Dockerfile
CMD ["python", "users_service.py"]
```
##### Commandes pour construire et publier l'image Docker :
```bash
docker build -t loris-egault/users-service:latest .
docker push loris-egault/users-service:latest
```
#### Variabilisation des param√®tres

Les param√®tres sensibles (comme les mots de passe et les URL des bases de donn√©es) sont externalis√©s sous forme de variables d'environnement pour am√©liorer la s√©curit√© et la portabilit√© :

##### Exemple dans le code Python :
```python
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
ADMIN_KEY = os.getenv("ADMIN_KEY", "default_key")
```
##### Exemple de fichier .env pour chaque environnement (qualification et production) :
```python
REDIS_HOST=redis-qualification
REDIS_PORT=6379
ADMIN_KEY=supersecretkey
```
#### Tests des microservices

*Chaque microservice est test√© ind√©pendamment sur une machine virtuelle avant le d√©ploiement* :

##### Installation des modules n√©cessaires :
```python
pip install flask redis flasgger wraps
```
##### Commande pour tester un microservice (/users) en local :
```python
python users_service.py
```
##### Exemple de requ√™te pour tester l'API /users :
```bash
curl -X POST http://localhost:5000/users -H "Authorization: supersecretkey" -d '{"id": "1", "name": "Tintin", "password": "milou"}'
curl -X GET http://localhost:5000/users -H "Authorization: supersecretkey"
```
## Lot 4 : Instanciation de l'application dans l'orchestrateur
## *Objectifs*

- D√©ployer les microservices dans des conteneurs sur deux plateformes cloisonn√©es : qualification et production.
- Configurer un orchestrateur de conteneurs (Kubernetes via Minikube) pour assurer la mont√©e en charge et la haute disponibilit√©.
- Int√©grer un reverse proxy pour g√©rer les requ√™tes entrantes et limiter les ressources allou√©es entre les plateformes.

### √âtapes de d√©ploiement

#### Cr√©ation des d√©ploiements et services Kubernetes

Chaque microservice est associ√© √† un d√©ploiement et un service expos√© dans Kubernetes :

##### D√©ploiement pour le service /users :
```python
apiVersion: apps/v1
kind: Deployment
metadata:
  name: users-deployment
  namespace: qualification
spec:
  replicas: 2
  selector:
    matchLabels:
      app: users
  template:
    metadata:
      labels:
        app: users
    spec:
      containers:
      - name: users
        image: loris-egault/users-service:latest
        ports:
        - containerPort: 5000
```
##### D√©ploiement pour le service /quotes :
```python
apiVersion: apps/v1
kind: Deployment
metadata:
  name: quotes-deployment
  namespace: qualification
spec:
  replicas: 2
  selector:
    matchLabels:
      app: quotes
  template:
    metadata:
      labels:
        app: quotes
    spec:
      containers:
      - name: quotes
        image: loris-egault/quotes-service:latest
        ports:
        - containerPort: 5000
```
##### D√©ploiement pour le service /search :
```python
apiVersion: apps/v1
kind: Deployment
metadata:
  name: search-deployment
  namespace: qualification
spec:
  replicas: 2
  selector:
    matchLabels:
      app: search
  template:
    metadata:
      labels:
        app: search
    spec:
      containers:
      - name: search
        image: loris-egault/search-service:latest
        ports:
        - containerPort: 5000
```
### Configuration du reverse proxy avec Traefik

Un fichier de configuration YAML doit √™tre cr√©√© pour int√©grer Traefik en tant que proxy invers√© :

##### Exemple de fichier traefik-ingress.yaml :
```python
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: traefik-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: qualification-app.nip.io
    http:
      paths:
      - path: /users
        pathType: Prefix
        backend:
          service:
            name: users-service
            port:
              number: 5000
      - path: /quotes
        pathType: Prefix
        backend:
          service:
            name: quotes-service
            port:
              number: 5000
```
### Gestion des ressources
```python
Allocation de quotas de CPU et de m√©moire pour les namespaces :

apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota
  namespace: production
spec:
  hard:
    requests.cpu: "2"
    requests.memory: 4Gi
    limits.cpu: "4"
    limits.memory: 8Gi
```
### Configuration des limites de requ√™tes

Traefik a √©t√© configur√© pour limiter les requ√™tes HTTP √† 10 par minute afin de prot√©ger les ressources.
## Lot 5 : Mise en place d'une couche d'observabilit√©
## *Objectifs*

- Fournir un tableau de bord d√©taill√© pour surveiller les performances de l'application et des plateformes.
- Mettre en place un m√©canisme d'alerte en cas de surcharge ou d'erreurs critiques.

### Outils utilis√©s

- Prometheus : pour collecter les m√©triques syst√®me et applicatives.
- Grafana : pour visualiser les m√©triques dans des tableaux de bord.
- AlertManager : pour configurer des alertes.

### √âtapes de mise en place
#### D√©ploiement de Prometheus

##### Exemple de fichier prometheus-deployment.yaml :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
```
#### Configuration de Grafana

##### Tableau de bord contenant :
- Le temps de r√©ponse des API.
- Le nombre de conteneurs en ex√©cution.
- La m√©moire et CPU utilis√©s.
- Le nombre de requ√™tes par service.

### M√©canisme d'alerte

#### Configuration d'AlertManager pour envoyer des alertes via email en cas de :

- Taux √©lev√© de requ√™tes √©chou√©es.
- Surcharge CPU ou m√©moire d√©passant un seuil d√©fini.
